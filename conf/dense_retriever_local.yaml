defaults:
  # Defines encoder initialization parameters
  - encoder: "hf_bert" 
  # contains a list of all possible sources of queries for 
  # evaluation. Specific set is selected by qa_dataset parameter
  - datasets: "retriever_default" 
  # contains a list of all possible passage sources. 
  # Specific passages sources selected by ctx_datatsets parameter
  - ctx_sources: "default_sources" 
  
indexers:
  flat:
    _target_: "dpr.indexer.faiss_indexers.DenseFlatIndexer"

  hnsw:
    _target_: "dpr.indexer.faiss_indexers.DenseHNSWFlatIndexer"

  hnsw_sq:
    _target_: "dpr.indexer.faiss_indexers.DenseHNSWSQIndexer"



out_file: 
validation_workers: 15
n_gpu: 1
# The name of the queries dataset from the 
# 'datasets' config group
qa_dataset: "nq_test"

# A list of names of the passages datasets 
# from the 'ctx_sources' config group
ctx_datatsets: ["dpr_wiki"]

# Glob paths to encoded passages 
# (from generate_dense_embeddings tool)

encoded_ctx_files: [
  "/Users/jules/Documents/Work/DecodingRetriever/mila/DPR/downloads/data/wikipedia_split/psgs_w100.tsv"
]
match: "string"  # "regex" or "string"
n_docs: 100
batch_size: 128  # Batch size to generate query embeddings

# Whether to lower case the input text. 
# Set True for uncased models, False for the cased ones.
do_lower_case: True

# The attribute name of encoder to use for queries. 
# Options for the BiEncoder model: question_model, ctx_model
# question_model is used if this param is empty
encoder_path:

# Path to the FAISS index location - it is only needed if you 
# want to serialize faiss index to files or read from them
# (instead of using encoded_ctx_files)
# it should point to either directory or a common index 
# files prefix name if there is no index at the specific 
# location, the index will be created from encoded_ctx_files
index_path: 
 "/Users/jules/Documents/Work/DecodingRetriever/mila/DPR/dpr/downloads/indexes/single/nq/full"

kilt_out_file:

# A trained bi-encoder checkpoint file to initialize the model
model_file: 
 "/Users/jules/Documents/Work/DecodingRetriever/mila/DPR/dpr/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp"

validate_as_tables: False
rpc_retriever_cfg_file:
indexer: "flat"

special_tokens:  # Tokens which won't be slit by tokenizer

# TODO: move to a conf group
# local_rank for distributed training on gpus
local_rank: -1
global_loss_buf_sz: 150000
device:
distributed_world_size:
no_cuda: False
fp16: False

# For fp16: 
# Apex AMP optimization level selected in 
# ['O0', 'O1', 'O2', and 'O3']."
#    "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: "O1"